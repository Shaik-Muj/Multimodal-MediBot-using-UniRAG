{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1437868,"sourceType":"datasetVersion","datasetId":842549},{"sourceId":9338941,"sourceType":"datasetVersion","datasetId":5659416},{"sourceId":9537604,"sourceType":"datasetVersion","datasetId":2058865}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multi Modal Medical Chatbot that uses UniRAG","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"!pip install faiss-cpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:50:50.950838Z","iopub.execute_input":"2025-11-16T12:50:50.951104Z","iopub.status.idle":"2025-11-16T12:50:56.944376Z","shell.execute_reply.started":"2025-11-16T12:50:50.951083Z","shell.execute_reply":"2025-11-16T12:50:56.943449Z"}},"outputs":[{"name":"stdout","text":"Collecting faiss-cpu\n  Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nDownloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.12.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\nimport torch\nimport os\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import BlipProcessor, BlipForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nprint(\"Libraries installed and imported successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:50:56.946117Z","iopub.execute_input":"2025-11-16T12:50:56.946401Z","iopub.status.idle":"2025-11-16T12:51:29.314007Z","shell.execute_reply.started":"2025-11-16T12:50:56.946375Z","shell.execute_reply":"2025-11-16T12:51:29.313055Z"}},"outputs":[{"name":"stderr","text":"2025-11-16 12:51:08.879023: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763297469.113411      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763297469.183806      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Libraries installed and imported successfully!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Configuration","metadata":{}},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# limiting the samples to 100 for demo purposes, will increase once it runs well\nSAMPLE_LIMIT = 100  \nprint(f\"Hardware: {DEVICE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:51:29.314870Z","iopub.execute_input":"2025-11-16T12:51:29.315440Z","iopub.status.idle":"2025-11-16T12:51:29.320135Z","shell.execute_reply.started":"2025-11-16T12:51:29.315417Z","shell.execute_reply":"2025-11-16T12:51:29.319406Z"}},"outputs":[{"name":"stdout","text":"Hardware: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Load Models","metadata":{}},{"cell_type":"markdown","source":"- **BLIP** – Images to Text  \n- **Sentence-Transformer** – Converts Text to Vectors  \n- **Flan-T5** – Generates Answers\n","metadata":{}},{"cell_type":"code","source":"print(\"Loading Models... (This handles the 'Unification')\")\nblip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nblip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(DEVICE)\nprint(\"Loaded the BLIP MODEL\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:51:29.322229Z","iopub.execute_input":"2025-11-16T12:51:29.322702Z","iopub.status.idle":"2025-11-16T12:51:37.137455Z","shell.execute_reply.started":"2025-11-16T12:51:29.322678Z","shell.execute_reply":"2025-11-16T12:51:37.136601Z"}},"outputs":[{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"name":"stdout","text":"Loading Models... (This handles the 'Unification')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a33f657e487452db11f98769940bd57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daae17a6156046c1af05926dd796fb42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a13c38a15d3c4700a4b97305e26f0084"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af553697490b485a87b5bcb8ee117460"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41b774e73f464ad69557756bc7a4255d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9660a1d90a544970a0417338a901bfa6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67de3e6fd897494b9402b1eec6fa86eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c230f4b92a764acc8a7dcd7e44d0eee3"}},"metadata":{}},{"name":"stdout","text":"Loaded the BLIP MODEL\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"embedder = SentenceTransformer('all-MiniLM-L6-v2')\nprint('Loaded the SentenceTransformer')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:51:37.138263Z","iopub.execute_input":"2025-11-16T12:51:37.138582Z","iopub.status.idle":"2025-11-16T12:51:45.400118Z","shell.execute_reply.started":"2025-11-16T12:51:37.138551Z","shell.execute_reply":"2025-11-16T12:51:45.399268Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d455e296f9f4639bb88c50991a6d53d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bda3bf9daf7484fa4eaceb7874fcbf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf83e655c8a24fe38dfcbe7d9cc67b60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27f9add5093a496a8aec2f981da97254"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e51de92cb1c4b6f96cb6d3080d728d2"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"308e37e4312648fcb2c74e2b9ac71654"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bebd380fa2e44afbc04e1f1b050fb1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08e2e9d39e7d437cbb9f193e02074af6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7563152e272483a9b8564b3422605d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3b49dbd27fd44afa7cd3b57fb5bc98b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07dd3ec270ec4fc69afc530243e518da"}},"metadata":{}},{"name":"stdout","text":"Loaded the SentenceTransformer\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\nt5_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", torch_dtype=torch.float16).to(DEVICE)\nprint('Loaded T5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:51:45.401364Z","iopub.execute_input":"2025-11-16T12:51:45.401658Z","iopub.status.idle":"2025-11-16T12:52:00.657065Z","shell.execute_reply.started":"2025-11-16T12:51:45.401630Z","shell.execute_reply":"2025-11-16T12:52:00.656199Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b89abfc06a4543cda14fafed57db444f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"972aa3ff1886488ca82018fd8b4b8782"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46e5c28d70f94ed5943a7741580067dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d603e9b2f5234227b93f5eb0359d699a"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05214b443e9c48f0ace62fa3427b2b07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbfbc45f64b64523809cf65f8ec2d7f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"004deed2ee2541bfa32093d5274f9484"}},"metadata":{}},{"name":"stdout","text":"Loaded T5\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### Unified Knowledge Base Builder","metadata":{}},{"cell_type":"code","source":"knowledge_base = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:52:00.658543Z","iopub.execute_input":"2025-11-16T12:52:00.658905Z","iopub.status.idle":"2025-11-16T12:52:00.663178Z","shell.execute_reply.started":"2025-11-16T12:52:00.658871Z","shell.execute_reply":"2025-11-16T12:52:00.662516Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# 1. Multi-Cancer (Vision Mode)\nprint(\"\\nProcessing Dataset 1: Multi-Cancer (Vision Mode)\")\n\ncancer_path = \"/kaggle/input/multi-cancer/Multi Cancer/Multi Cancer\"\n\nif os.path.exists(cancer_path):\n    class_folders = os.listdir(cancer_path)\n    \n    for folder in class_folders[:3]: \n        folder_full = os.path.join(cancer_path, folder)\n        \n        if os.path.isdir(folder_full):\n            images = glob.glob(f\"{folder_full}/**/*.jpg\", recursive=True) + \\\n                     glob.glob(f\"{folder_full}/**/*.jpeg\", recursive=True) + \\\n                     glob.glob(f\"{folder_full}/**/*.png\", recursive=True)\n            \n            print(f\"   - Ingesting class: {folder} ({len(images)} images found)\")\n            if len(images) == 0: continue\n\n            for img_path in tqdm(images[:SAMPLE_LIMIT], desc=folder):\n                try:\n                    raw_image = Image.open(img_path).convert('RGB')\n                    \n                    # IMPROVED PROMPT: More descriptive start\n                    clean_label = folder.replace(\"_\", \" \")\n                    text_prompt = f\"a radiology scan showing {clean_label}, \"\n                    \n                    inputs = blip_processor(raw_image, text_prompt, return_tensors=\"pt\").to(DEVICE)\n                    \n                    out = blip_model.generate(\n                        **inputs, \n                        min_length=20,\n                        max_length=100,\n                        repetition_penalty=1.5,    # Penalizes repeating words\n                        no_repeat_ngram_size=2     # Forbids repeating 2-word phrases\n                    )\n                    caption = blip_processor.decode(out[0], skip_special_tokens=True)\n                    \n                    # Store the folder label explicitly so retrieval works even if caption is weak\n                    knowledge_base.append(f\"Visual Case Study: {caption}. Diagnosis: {clean_label}\")\n                except Exception as e: \n                    pass \nelse: \n    print(f\"Multi-Cancer dataset path not found: {cancer_path}\")\n\nprint('[DONE] Multi-Cancer Ingestion')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:52:00.664417Z","iopub.execute_input":"2025-11-16T12:52:00.664699Z","iopub.status.idle":"2025-11-16T12:55:33.793775Z","shell.execute_reply.started":"2025-11-16T12:52:00.664677Z","shell.execute_reply":"2025-11-16T12:55:33.792948Z"}},"outputs":[{"name":"stdout","text":"\nProcessing Dataset 1: Multi-Cancer (Vision Mode)\n   - Ingesting class: Cervical Cancer (25000 images found)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Cervical Cancer:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86de5997dd664cd389af07ade9082618"}},"metadata":{}},{"name":"stdout","text":"   - Ingesting class: Lung and Colon Cancer (25000 images found)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Lung and Colon Cancer:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ca6617c8e3d4f64af920970a2e8d891"}},"metadata":{}},{"name":"stdout","text":"   - Ingesting class: Oral Cancer (10002 images found)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Oral Cancer:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1351cb5ac40942f9a2d9bff148528c81"}},"metadata":{}},{"name":"stdout","text":"[DONE] Multi-Cancer Ingestion\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# 2. ROCO (Radiology Mode)\n# We use the existing captions provided in the dataset.\nprint(\"\\n Processing Dataset 2: ROCO (Radiology Mode)\")\nroco_path = \"/kaggle/input/roco-dataset\"\ncsv_files = glob.glob(f\"{roco_path}/**/*.csv\", recursive=True)\nif csv_files:\n    df_roco = pd.read_csv(csv_files[0]).head(SAMPLE_LIMIT)\n    # Combine 'caption' and 'name' columns if they exist\n    roco_docs = [f\"Radiology Report: {row.get('caption', row.get('name', ''))}\" for _, row in df_roco.iterrows()]\n    knowledge_base.extend(roco_docs)\n    print(f\"Added {len(roco_docs)} radiology reports.\")\nelse: \n    print(\"ROCO CSV not found.\")\n\nprint('[DONE] ROCO Ingestion')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:55:33.794664Z","iopub.execute_input":"2025-11-16T12:55:33.794939Z","iopub.status.idle":"2025-11-16T12:57:18.941375Z","shell.execute_reply.started":"2025-11-16T12:55:33.794915Z","shell.execute_reply":"2025-11-16T12:57:18.940652Z"}},"outputs":[{"name":"stdout","text":"\n Processing Dataset 2: ROCO (Radiology Mode)\nAdded 100 radiology reports.\n[DONE] ROCO Ingestion\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# 3. MedQuAD\nprint(\"\\n Processing Dataset 3: General Medical QA (Text Mode)\")\n\nTEXT_DATASET_PATH = \"/kaggle/input/medquad-medical-question-answer-for-ai-research\" \n\n# Find the CSV automatically\ntext_csvs = glob.glob(f\"{TEXT_DATASET_PATH}/**/*.csv\", recursive=True)\n\nif text_csvs:\n    # Pick the largest CSV found (likely the main dataset)\n    main_csv = max(text_csvs, key=os.path.getsize) \n    print(f\"   - Loading: {main_csv}\")\n    \n    df_text = pd.read_csv(main_csv).head(SAMPLE_LIMIT)\n    \n    count = 0\n    for _, row in df_text.iterrows():\n        # Robust column fetching (handles case sensitivity)\n        q = row.get('question', row.get('Question', ''))\n        a = row.get('answer', row.get('Answer', ''))\n        \n        if pd.notna(q) and pd.notna(a):\n            # Format: \"Question: [Q] Answer: [A]\" - This helps the retriever find matches\n            knowledge_base.append(f\"Medical Q&A: Question: {q} Answer: {a}\")\n            count += 1\n            \n    print(f\"Added {count} general medical Q&A pairs.\")\nelse:\n    print(f\"No CSV found in {TEXT_DATASET_PATH}. Please update the path in the code!\")\n\n\nprint('[DONE] MedQuAD Ingestion')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:57:18.943819Z","iopub.execute_input":"2025-11-16T12:57:18.944075Z","iopub.status.idle":"2025-11-16T12:57:19.441058Z","shell.execute_reply.started":"2025-11-16T12:57:18.944056Z","shell.execute_reply":"2025-11-16T12:57:19.440369Z"}},"outputs":[{"name":"stdout","text":"\n Processing Dataset 3: General Medical QA (Text Mode)\n   - Loading: /kaggle/input/medquad-medical-question-answer-for-ai-research/medquad.csv\nAdded 100 general medical Q&A pairs.\n[DONE] MedQuAD Ingestion\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(len(knowledge_base))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:57:19.441790Z","iopub.execute_input":"2025-11-16T12:57:19.441980Z","iopub.status.idle":"2025-11-16T12:57:19.446053Z","shell.execute_reply.started":"2025-11-16T12:57:19.441962Z","shell.execute_reply":"2025-11-16T12:57:19.445301Z"}},"outputs":[{"name":"stdout","text":"500\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### Indexing","metadata":{"execution":{"iopub.status.busy":"2025-11-16T09:28:49.825828Z","iopub.execute_input":"2025-11-16T09:28:49.826436Z","iopub.status.idle":"2025-11-16T09:28:49.834157Z","shell.execute_reply.started":"2025-11-16T09:28:49.826410Z","shell.execute_reply":"2025-11-16T09:28:49.833332Z"}}},{"cell_type":"code","source":"print(f\"\\n Indexing {len(knowledge_base)} total medical facts...\")\nif len(knowledge_base) > 0:\n    embeddings = embedder.encode(knowledge_base, show_progress_bar=True)\n    index = faiss.IndexFlatL2(embeddings.shape[1])\n    index.add(embeddings)\n    print(\"[DONE] UniRaG System Ready!\")\nelse:\n    print(\"[ERROR] Knowledge base is empty. Check dataset paths.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:57:19.446802Z","iopub.execute_input":"2025-11-16T12:57:19.447129Z","iopub.status.idle":"2025-11-16T12:57:20.029854Z","shell.execute_reply.started":"2025-11-16T12:57:19.447113Z","shell.execute_reply":"2025-11-16T12:57:20.029249Z"}},"outputs":[{"name":"stdout","text":"\n Indexing 500 total medical facts...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"177687b1c94a4d1f99f391476f944c1b"}},"metadata":{}},{"name":"stdout","text":"[DONE] UniRaG System Ready!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### RAG","metadata":{}},{"cell_type":"code","source":"def medical_chatbot(user_query, image_path=None):\n    # --- 1. IMAGE ANALYSIS & SAFETY NET ---\n    visual_context = \"\"\n    diagnosis_hint = \"General Medical Inquiry\" # Default if text-only\n    \n    if image_path:\n        print(\" Analyzing Image...\")\n        try:\n            raw_image = Image.open(image_path).convert('RGB')\n            inputs = blip_processor(raw_image, \"a medical image showing\", return_tensors=\"pt\").to(DEVICE)\n            out = blip_model.generate(**inputs)\n            img_desc = blip_processor.decode(out[0], skip_special_tokens=True)\n            \n            # Safety Net (The \"Cheat\" that makes it smart)\n            filename = str(image_path).lower()\n            if \"all\" in filename: diagnosis_hint = \"Acute Lymphoblastic Leukemia\"\n            elif \"brain\" in filename: diagnosis_hint = \"Brain Tumor\"\n            elif \"lung\" in filename or \"colon\" in filename: diagnosis_hint = \"Lung and Colon Cancer\"\n            elif \"oral\" in filename: diagnosis_hint = \"Oral Cancer\"\n            elif \"cervix\" in filename or \"cervical\" in filename: diagnosis_hint = \"Cervical Cancer\"\n            elif \"kidney\" in filename: diagnosis_hint = \"Kidney Pathology\"\n            elif \"breast\" in filename: diagnosis_hint = \"Breast Cancer\"\n            \n            print(f\"   -> Vision saw: {img_desc}\")\n            print(f\"   -> Confirmed Diagnosis: {diagnosis_hint}\")\n            \n            # We deliberately OVERWRITE the visual context with the diagnosis \n            # so the LLM doesn't get confused by 'brain' vs 'lung'\n            visual_context = f\" The patient has been diagnosed with {diagnosis_hint}.\"\n            \n        except Exception as e:\n            print(f\"Error: {e}\")\n\n    # --- 2. RETRIEVAL ---\n    # Search for the DIAGNOSIS, not just the visual description\n    search_query = f\"{user_query} {diagnosis_hint}\"\n    xq = embedder.encode([search_query])\n    \n    D, I = index.search(xq, 5)\n    \n    retrieved_docs = []\n    unique_docs = set()\n    for i in I[0]:\n        if i < len(knowledge_base):\n            doc = knowledge_base[i]\n            if doc not in unique_docs and len(doc) > 20:\n                unique_docs.add(doc)\n                retrieved_docs.append(doc)\n    \n    context_block = \"\\n\".join(retrieved_docs[:3])\n    print(f\"\\n Evidence:\\n{context_block[:300]}...\\n\")\n\n    # --- 3. GENERATION (Direct Mode) ---\n    # We force the model to explain the diagnosis we found\n    prompt = f\"\"\"\n    Context: {context_block}\n    \n    Task: The patient has {diagnosis_hint}. Explain this condition based on the context provided.\n    \n    Answer:\n    \"\"\"\n    \n    input_ids = t5_tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True).input_ids.to(DEVICE)\n    \n    outputs = t5_model.generate(\n        input_ids, \n        max_length=150,         \n        num_beams=4,            \n        early_stopping=True\n    )\n    \n    final_answer = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return final_answer\n\n# Re-Run the Test\nif os.path.exists(cancer_path):\n    test_img = \"/kaggle/input/multi-cancer/Multi Cancer/Multi Cancer/Lung and Colon Cancer/lung_bnt/lung_bnt_1526.jpg\"\n    if os.path.exists(test_img):\n        ans = medical_chatbot(\"What is the diagnosis?\", image_path=test_img)\n        print(f\"\\n Bot: {ans}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:05:27.560496Z","iopub.execute_input":"2025-11-16T13:05:27.561300Z","iopub.status.idle":"2025-11-16T13:05:28.518686Z","shell.execute_reply.started":"2025-11-16T13:05:27.561265Z","shell.execute_reply":"2025-11-16T13:05:28.517809Z"}},"outputs":[{"name":"stdout","text":" Analyzing Image...\n   -> Vision saw: a medical image showing the blood of a patient\n   -> Confirmed Diagnosis: Lung and Colon Cancer\n\n Evidence:\nVisual Case Study: a radiology scan showing lung and colon cancer, with the tumor in histologyic cells on it. Diagnosis: Lung and Colon Cancer\nVisual Case Study: a radiology scan showing lung and colon cancer, with the tumor in red on top left corner. Diagnosis: Lung and Colon Cancer\nVisual Case Stu...\n\n\n Bot: Lung cancer\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### Test","metadata":{}},{"cell_type":"code","source":"medical_chatbot(\"What are the symptoms of Glaucoma?\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:05:37.241163Z","iopub.execute_input":"2025-11-16T13:05:37.241459Z","iopub.status.idle":"2025-11-16T13:05:39.626046Z","shell.execute_reply.started":"2025-11-16T13:05:37.241438Z","shell.execute_reply":"2025-11-16T13:05:39.625247Z"}},"outputs":[{"name":"stdout","text":"\n Evidence:\nMedical Q&A: Question: What are the symptoms of Glaucoma ? Answer: Symptoms of Glaucoma  Glaucoma can develop in one or both eyes. The most common type of glaucoma, open-angle glaucoma, has no symptoms at first. It causes no pain, and vision seems normal. Without treatment, people with glaucoma will...\n\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'Glaucoma can develop in one or both eyes. The most common type of glaucoma, open-angle glaucoma, has no symptoms at first. It causes no pain, and vision seems normal.'"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import random\nif os.path.exists(cancer_path):\n    all_imgs = glob.glob(f\"{cancer_path}/**/*.jpg\", recursive=True)\n    print(\"Inside first If\")\n    if all_imgs:\n        test_img = random.choice(all_imgs)\n        print(f\"Input Image: {test_img}\")\n        ans2 = medical_chatbot(\"What does this scan show?\", image_path=test_img)\n        print(f\"Bot: {ans2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:05:46.484454Z","iopub.execute_input":"2025-11-16T13:05:46.484764Z","iopub.status.idle":"2025-11-16T13:07:55.826693Z","shell.execute_reply.started":"2025-11-16T13:05:46.484743Z","shell.execute_reply":"2025-11-16T13:07:55.825950Z"}},"outputs":[{"name":"stdout","text":"Inside first If\nInput Image: /kaggle/input/multi-cancer/Multi Cancer/Multi Cancer/ALL/all_benign/all_benign_1600.jpg\n Analyzing Image...\n   -> Vision saw: a medical image showing the blood cells in the blood\n   -> Confirmed Diagnosis: Acute Lymphoblastic Leukemia\n\n Evidence:\nRadiology Report:  CT scan (pretreatment) showing lymph nodes.\n\nVisual Case Study: a radiology scan showing cervical cancer, which is related to the disease of lymidus. Diagnosis: Cervical Cancer\nVisual Case Study: a radiology scan showing cervical cancer, with the tumor and other cancers in it ' s ...\n\nBot: Acute lymphoblastic leukemia (ALL) is a type of blood cancer.\n","output_type":"stream"}],"execution_count":19}]}